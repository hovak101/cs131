{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "428d2052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.5\n",
      "0.10.9.7\n",
      "openjdk version \"1.8.0_432-432\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_432-432-b06)\n",
      "OpenJDK 64-Bit Server VM (build 25.432-b06, mixed mode)\n",
      "\n",
      "java version \"17.0.12\" 2024-07-16 LTS\n",
      "Java(TM) SE Runtime Environment (build 17.0.12+8-LTS-286)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 17.0.12+8-LTS-286, mixed mode, sharing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "import py4j\n",
    "print(py4j.__version__)\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run('java -version', capture_output=True, text=True, shell=True)\n",
    "print(result.stderr)\n",
    "\n",
    "import os\n",
    "\n",
    "# only for running locally. \n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-17\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "result = subprocess.run('java -version', capture_output=True, text=True, shell=True)\n",
    "print(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89e988b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkSQLApp\").master(\"local[*]\").config(\"spark.driver.host\", \"localhost\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()\n",
    "\n",
    "csv_file = \"C:/Users/hovak/OneDrive/Desktop/a4-spark/2019-01-h1.csv\"\n",
    "\n",
    "df = (spark.read.format(\"csv\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(csv_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76752ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of the working DataFrame:\n",
      "+---------------+------------+------------+------------+\n",
      "|passenger_count|pulocationid|dolocationid|total_amount|\n",
      "+---------------+------------+------------+------------+\n",
      "|1.0            |151.0       |239.0       |9.95        |\n",
      "|1.0            |239.0       |246.0       |16.3        |\n",
      "|3.0            |236.0       |236.0       |5.8         |\n",
      "|5.0            |193.0       |193.0       |7.55        |\n",
      "|5.0            |193.0       |193.0       |55.55       |\n",
      "|5.0            |193.0       |193.0       |13.31       |\n",
      "|5.0            |193.0       |193.0       |55.55       |\n",
      "|1.0            |163.0       |229.0       |9.05        |\n",
      "|1.0            |229.0       |7.0         |18.5        |\n",
      "|2.0            |141.0       |234.0       |13.0        |\n",
      "+---------------+------------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_cols = [\"passenger_count\", \"pulocationid\", \"dolocationid\", \"total_amount\"]\n",
    "taxiDF = df.select(*my_cols)\n",
    "print(\"First 10 rows of the working DataFrame:\")\n",
    "taxiDF.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42c1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF, testDF = taxiDF.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30d50545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"passenger_count\", \"pulocationid\", \"dolocationid\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "dtr = (DecisionTreeRegressor(labelCol=\"total_amount\",\n",
    "                             featuresCol=\"features\",\n",
    "                             maxDepth=10)           \n",
    "       .setMaxBins(700))         \n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, dtr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d738771",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a2b6c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features and prediction sample:\n",
      "+---------------+------------+------------+------------------+------------+\n",
      "|passenger_count|pulocationid|dolocationid|prediction        |total_amount|\n",
      "+---------------+------------+------------+------------------+------------+\n",
      "|0.0            |4.0         |4.0         |10.370287081339715|5.75        |\n",
      "|0.0            |4.0         |90.0        |14.11333610418399 |12.3        |\n",
      "|0.0            |4.0         |144.0       |14.11333610418399 |9.45        |\n",
      "|0.0            |7.0         |7.0         |7.537342592592594 |5.8         |\n",
      "|0.0            |7.0         |112.0       |14.11333610418399 |16.8        |\n",
      "|0.0            |7.0         |164.0       |14.11333610418399 |21.8        |\n",
      "|0.0            |12.0        |45.0        |17.034383527033853|10.8        |\n",
      "|0.0            |13.0        |13.0        |13.741151802656542|9.95        |\n",
      "|0.0            |13.0        |164.0       |19.998085075066232|18.35       |\n",
      "|0.0            |13.0        |164.0       |19.998085075066232|21.3        |\n",
      "+---------------+------------+------------+------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(testDF)\n",
    "\n",
    "print(\"Features and prediction sample:\")\n",
    "(predictions\n",
    "   .select(\"passenger_count\", \"pulocationid\", \"dolocationid\",\n",
    "           \"prediction\", \"total_amount\")\n",
    "   .show(10, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "400ef220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-Mean-Squared-Error (RMSE): 77.21\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"total_amount\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root-Mean-Squared-Error (RMSE): {rmse:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
